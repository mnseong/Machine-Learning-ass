{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from tensorflow.keras.metrics import categorical_accuracy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPU를 이용하기 위해 device를 환경에 맞게끔 초기화 해준다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS enabled.\n"
     ]
    }
   ],
   "source": [
    "# CUDA를 이용한 GPU 가속\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# Mac M1에서 Pytorch 를 이용할 때 GPU 연산 가속을 위한 MPS\n",
    "elif torch.torch.backends.mps.is_available():\n",
    "    device = torch.device('mps:0')\n",
    "\n",
    "    print(\"MPS enabled.\")\n",
    "\n",
    "# CPU 이용\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data를 학습시킬 때 필요한 여러 parameter 들을 config로 미리 정의"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':30,\n",
    "    'LEARNING_RATE':8e-3,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델의 성능을 재현하기 위해 seed값 고정"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "torch.manual_seed(CFG['SEED'])\n",
    "np.random.seed(CFG['SEED'])\n",
    "random.seed(CFG['SEED'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# IRIS dataset 불러오기\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "labels = iris.target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "load_iris()를 사용하여 불러온 것과 pandas를 활용해 새롭게 데이터 프레임을 구성"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                  5.1               3.5                1.4               0.2   \n1                  4.9               3.0                1.4               0.2   \n2                  4.7               3.2                1.3               0.2   \n3                  4.6               3.1                1.5               0.2   \n4                  5.0               3.6                1.4               0.2   \n..                 ...               ...                ...               ...   \n145                6.7               3.0                5.2               2.3   \n146                6.3               2.5                5.0               1.9   \n147                6.5               3.0                5.2               2.0   \n148                6.2               3.4                5.4               2.3   \n149                5.9               3.0                5.1               1.8   \n\n     target  \n0       0.0  \n1       0.0  \n2       0.0  \n3       0.0  \n4       0.0  \n..      ...  \n145     2.0  \n146     2.0  \n147     2.0  \n148     2.0  \n149     2.0  \n\n[150 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3.0</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3.0</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%ㅑ\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0    50\n1.0    50\n2.0    50\nName: target, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Target class는 총 3개이며, 각 class는 50개의 데이터로 구성되어 있으므로, 전체 데이터는 balance 하다고 볼 수 있다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train_test_split을 이용하여, train, validation, test dataset으로 분리한다.\n",
    "먼저 train과 test를 8:2 의 비율로 나누고,\n",
    "나눈 train을 다시 4:1의 비율로 train과 validation으로 나누어,\n",
    "최종적으로 전체 dataset에 대해 train:validation:test = 6:2:2의 비율로 나누었다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=CFG['SEED'])\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=CFG['SEED'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch의 Dataset을 상속받아서 CustomDataset을 구현한다.\n",
    "Dataset의 feature와 label을 input 값으로 입력받아 tensor로 변형하여 return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.features[index], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_train, y_train)\n",
    "val_dataset = CustomDataset(x_val, y_val)\n",
    "test_dataset = CustomDataset(x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch의 DataLoader를 이용해서, 각각의 dataset을 이용해서 data loader를 정의"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "총 3개의 은닉충을 사용하여 딥러닝 모델을 구현하였다.\n",
    "처음 input layer에서 4개의 feature 들을 512개의 차원으로 변형하고,\n",
    "은닉층을 3개 거치면서, 절반의 크기의 dimension으로 변형되어 학습된다.\n",
    "마지막으로 output layer에서 64개의 dimension을 target class 개수인 3개 변형되어 return 된다.\n",
    "각각의 layer를 거칠 때 마다 relu 라는 활성화함수를 거쳐, 그 다음 layer로 넘겨준다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(MyModel,self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, 512)\n",
    "        self.hidden_layer1 = nn.Linear(512, 256)\n",
    "        self.hidden_layer2 = nn.Linear(256, 128)\n",
    "        self.hidden_layer3 = nn.Linear(128, 64)\n",
    "        self.output_layer = nn.Linear(64,output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 입력층\n",
    "        out = self.input_layer(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # 은닉층\n",
    "        out = self.hidden_layer1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.hidden_layer2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.hidden_layer3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # 출력층\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "model과 train, val loader 그리고 criterion(loss function) optimizer, epochs number 등을 파라미터로 받아 학습시키는 train함수를 정의한다.\n",
    "epochs수 만큼 돌아가며 학습을 시키며, 그때마다 accuracy를 계산하여 가장 좋은 accuracy로 계속 갱신한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        val_loss = []\n",
    "        preds, trues = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # optimizer 행렬 0으로 초기화\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # loss를 계산하여 역전파\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                logit = model(inputs)\n",
    "                loss = criterion(logit, labels)\n",
    "\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "                preds += logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "                trues += labels.detach().cpu().numpy().tolist()\n",
    "\n",
    "            _val_loss = np.mean(val_loss)\n",
    "\n",
    "        # _val_categorical_accuracy = np.mean(categorical_accuracy(trues, preds))\n",
    "        _val_accuracy = accuracy_score(trues, preds)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {_val_loss:.4f}, \"\n",
    "              f\"Val Accuracy: {_val_accuracy:.4f}\")\n",
    "\n",
    "        # loss값이 이전까지 loss값의 최소값보다 작을 경우, 가장 좋은 성능의 model로 갱신한다.\n",
    "        if _val_loss < best_val_loss:\n",
    "            best_val_loss = _val_loss\n",
    "            best_model_state_dict = model.state_dict()\n",
    "\n",
    "    # best model을 불러와서 return\n",
    "    model.load_state_dict(best_model_state_dict)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    test_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    num_samples = len(test_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_accuracy = accuracy / num_samples * 100\n",
    "\n",
    "    print(f\"Test Accuracy: {avg_accuracy:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "features 의 개수가 4개고, target class의 개수가 3개이므로, 입력층의 dimension을 4, 출력층의 dimension을 3 으로 지정하여 model을 정의해준다.\n",
    "정의한 model을 GPU로 연산하기 위해, model.to(device)를 해준다.\n",
    "loss function은 CrossEntropyLoss를 사용하고,\n",
    "optimizer는 SGD를 사용하였다. 이때 momentum의 값은 0.9로 한다.\n",
    "learning_rate는 8e-3, EPOCHS수는 30으로 정하여 train 시킨다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.1033, Val Loss: 1.0821, Val Accuracy: 0.3000\n",
      "Epoch 2: Train Loss: 1.0603, Val Loss: 1.0393, Val Accuracy: 0.5667\n",
      "Epoch 3: Train Loss: 0.9898, Val Loss: 0.9948, Val Accuracy: 0.5667\n",
      "Epoch 4: Train Loss: 0.8825, Val Loss: 0.8631, Val Accuracy: 0.5667\n",
      "Epoch 5: Train Loss: 0.6794, Val Loss: 0.6324, Val Accuracy: 0.5667\n",
      "Epoch 6: Train Loss: 0.4894, Val Loss: 0.4776, Val Accuracy: 0.8667\n",
      "Epoch 7: Train Loss: 0.4107, Val Loss: 0.4477, Val Accuracy: 0.7000\n",
      "Epoch 8: Train Loss: 0.3289, Val Loss: 0.4197, Val Accuracy: 0.7333\n",
      "Epoch 9: Train Loss: 0.2446, Val Loss: 0.6397, Val Accuracy: 0.6000\n",
      "Epoch 10: Train Loss: 0.2136, Val Loss: 0.2865, Val Accuracy: 0.8667\n",
      "Epoch 11: Train Loss: 0.2576, Val Loss: 0.6028, Val Accuracy: 0.7333\n",
      "Epoch 12: Train Loss: 0.4211, Val Loss: 0.5837, Val Accuracy: 0.7333\n",
      "Epoch 13: Train Loss: 0.2865, Val Loss: 0.2027, Val Accuracy: 0.9333\n",
      "Epoch 14: Train Loss: 0.5163, Val Loss: 0.9919, Val Accuracy: 0.5667\n",
      "Epoch 15: Train Loss: 0.5363, Val Loss: 0.5422, Val Accuracy: 0.7333\n",
      "Epoch 16: Train Loss: 0.5161, Val Loss: 0.5052, Val Accuracy: 0.7000\n",
      "Epoch 17: Train Loss: 0.4135, Val Loss: 0.5531, Val Accuracy: 0.6000\n",
      "Epoch 18: Train Loss: 0.3748, Val Loss: 0.4460, Val Accuracy: 0.9000\n",
      "Epoch 19: Train Loss: 0.3303, Val Loss: 0.4328, Val Accuracy: 0.7333\n",
      "Epoch 20: Train Loss: 0.3034, Val Loss: 0.3037, Val Accuracy: 0.9333\n",
      "Epoch 21: Train Loss: 0.2051, Val Loss: 0.2619, Val Accuracy: 0.9000\n",
      "Epoch 22: Train Loss: 0.1465, Val Loss: 0.1914, Val Accuracy: 0.9333\n",
      "Epoch 23: Train Loss: 0.0953, Val Loss: 0.1712, Val Accuracy: 0.9333\n",
      "Epoch 24: Train Loss: 0.3020, Val Loss: 0.1475, Val Accuracy: 0.9333\n",
      "Epoch 25: Train Loss: 0.6621, Val Loss: 0.4941, Val Accuracy: 0.7667\n",
      "Epoch 26: Train Loss: 0.6229, Val Loss: 0.6095, Val Accuracy: 0.7000\n",
      "Epoch 27: Train Loss: 0.5389, Val Loss: 0.4607, Val Accuracy: 0.9000\n",
      "Epoch 28: Train Loss: 0.4302, Val Loss: 0.5256, Val Accuracy: 0.5667\n",
      "Epoch 29: Train Loss: 0.4008, Val Loss: 0.4403, Val Accuracy: 0.6333\n",
      "Epoch 30: Train Loss: 0.3650, Val Loss: 0.3903, Val Accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": "MyModel(\n  (input_layer): Linear(in_features=4, out_features=512, bias=True)\n  (hidden_layer1): Linear(in_features=512, out_features=256, bias=True)\n  (hidden_layer2): Linear(in_features=256, out_features=128, bias=True)\n  (hidden_layer3): Linear(in_features=128, out_features=64, bias=True)\n  (output_layer): Linear(in_features=64, out_features=3, bias=True)\n  (relu): ReLU()\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(4, 3)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=CFG['LEARNING_RATE'], momentum=0.9)\n",
    "\n",
    "train(model, train_loader, val_loader, criterion, optimizer, CFG['EPOCHS'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습을 시켜, 각 epochs마다 loss값과 accuracy를 계산하여 계속 학습시키니, 점차 loss값이 줄어들고, accuracy값은 상승하는 것을 볼 수 있다.\n",
    "train loss만 줄어들고, validation loss가 먼저 높은 값에서 수렴한다면, over fitting이 된 것이라 해석할 수 있겠으나,\n",
    "train loss와 validation loss 가 함께 줄어들며 학습되고 있으므로, model이 잘 학습되었음을 알 수 있다.\n",
    "처음에 learning rate를 1e-2로 하여 학습시켰으나, loss 값이 안정적으로 수렴하는 모습을 보이지않아, 차츰 줄여가며 학습시켜보았다.\n",
    "최종적으로 8e-3을 learning rate로 선정하였으며, 1e-3으로 하니 너무 일찍 validation loss가 수렴하여, 원하는 성능이 나오지 않았다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader, criterion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습한 model을 test dataset에 대하여 검증해보니, 96.67%의 accuracy를 달성할 수 있었다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
