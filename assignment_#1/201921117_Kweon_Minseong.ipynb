{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UCI Dataset 중, Wine dataset을 사용하여 ML model에 학습시켜보겠다.\n",
    "먼저, sklearn 라이브러리의 load_wine을 이용하여 dataset을 불러온다.\n",
    "ML model을 이용하여 classification 하는 task를 해볼 것이다.\n",
    "wine의 여러 feature를 학습하여 와인의 품질을 분류하는 것이 충분히 가능하기 때문이다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "    alcohol malic_acid   ash alcalinity_of_ash magnesium total_phenols  \\\n0     14.23       1.71  2.43              15.6     127.0          2.80   \n1     13.20       1.78  2.14              11.2     100.0          2.65   \n2     13.16       2.36  2.67              18.6     101.0          2.80   \n3     14.37       1.95  2.50              16.8     113.0          3.85   \n4     13.24       2.59  2.87              21.0     118.0          2.80   \n..      ...        ...   ...               ...       ...           ...   \n173   13.71       5.65  2.45              20.5      95.0          1.68   \n174   13.40       3.91  2.48              23.0     102.0          1.80   \n175   13.27       4.28  2.26              20.0     120.0          1.59   \n176   13.17       2.59  2.37              20.0     120.0          1.65   \n177   14.13       4.10  2.74              24.5      96.0          2.05   \n\n    flavanoids nonflavanoid_phenols proanthocyanins color_intensity   hue  \\\n0         3.06                 0.28            2.29            5.64  1.04   \n1         2.76                 0.26            1.28            4.38  1.05   \n2         3.24                 0.30            2.81            5.68  1.03   \n3         3.49                 0.24            2.18            7.80  0.86   \n4         2.69                 0.39            1.82            4.32  1.04   \n..         ...                  ...             ...             ...   ...   \n173       0.61                 0.52            1.06            7.70  0.64   \n174       0.75                 0.43            1.41            7.30  0.70   \n175       0.69                 0.43            1.35           10.20  0.59   \n176       0.68                 0.53            1.46            9.30  0.60   \n177       0.76                 0.56            1.35            9.20  0.61   \n\n    od280/od315_of_diluted_wines proline target  \n0                           3.92  1065.0      0  \n1                           3.40  1050.0      0  \n2                           3.17  1185.0      0  \n3                           3.45  1480.0      0  \n4                           2.93   735.0      0  \n..                           ...     ...    ...  \n173                         1.74   740.0      2  \n174                         1.56   750.0      2  \n175                         1.56   835.0      2  \n176                         1.62   840.0      2  \n177                         1.60   560.0      2  \n\n[178 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95.0</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>13.40</td>\n      <td>3.91</td>\n      <td>2.48</td>\n      <td>23.0</td>\n      <td>102.0</td>\n      <td>1.80</td>\n      <td>0.75</td>\n      <td>0.43</td>\n      <td>1.41</td>\n      <td>7.30</td>\n      <td>0.70</td>\n      <td>1.56</td>\n      <td>750.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>13.27</td>\n      <td>4.28</td>\n      <td>2.26</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.59</td>\n      <td>0.69</td>\n      <td>0.43</td>\n      <td>1.35</td>\n      <td>10.20</td>\n      <td>0.59</td>\n      <td>1.56</td>\n      <td>835.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>13.17</td>\n      <td>2.59</td>\n      <td>2.37</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.65</td>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>1.46</td>\n      <td>9.30</td>\n      <td>0.60</td>\n      <td>1.62</td>\n      <td>840.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>14.13</td>\n      <td>4.10</td>\n      <td>2.74</td>\n      <td>24.5</td>\n      <td>96.0</td>\n      <td>2.05</td>\n      <td>0.76</td>\n      <td>0.56</td>\n      <td>1.35</td>\n      <td>9.20</td>\n      <td>0.61</td>\n      <td>1.60</td>\n      <td>560.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>178 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = load_wine()\n",
    "wine_data = wine.data\n",
    "df_wine = pd.DataFrame(data = wine_data, columns=[wine.feature_names])\n",
    "df_wine['target'] = wine.target\n",
    "df_wine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "UCI dataset의 wine dataset 에는 총 13가지의 feature class가 있으며, target값을 데이터프레임에 붙여 총 14개의 columns을 가진 데이터 프레임이 되었다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[('alcohol',),\n ('malic_acid',),\n ('ash',),\n ('alcalinity_of_ash',),\n ('magnesium',),\n ('total_phenols',),\n ('flavanoids',),\n ('nonflavanoid_phenols',),\n ('proanthocyanins',),\n ('color_intensity',),\n ('hue',),\n ('od280/od315_of_diluted_wines',),\n ('proline',),\n ('target',)]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.columns.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "wine의 target 값은 wine의 품질이며, {0, 1, 2}의 세가지 class로 분류되어 있다. class 분포는 약간 imbalance한 편이다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(target,)\n1            71\n0            59\n2            48\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine['target'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gaussian Naive Bayes, DecisionTree, RandomForest 총 3가지 model을 사용하여 학습시킬 것 이다.\n",
    "sklearn 라이브러리에서 알맞는 model들을 불러온다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "dtc = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sklearn 라이브러리의 train_test_split을 사용하여 8:2의 비율로 train과 test를 나누어서 학습과 추론과정을 진행할 것 이다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(wine.data, wine.target, stratify = df_wine['target'], random_state=42, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "각각의 모델들에 대해서 Train dataset으로 학습시킨 뒤, Test dataset으로 검증한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Train\n",
    "nb.fit(train_X, train_y)\n",
    "dtc.fit(train_X, train_y)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "# Inference\n",
    "pred_nb = nb.predict(test_X)\n",
    "pred_dtc = dtc.predict(test_X)\n",
    "pred_rf = rf.predict(test_X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습된 결과에 대해서 Confusion Matrix(혼동 행렬)을 뽑아봐서 전체 class 중 어느 정도로 제대로 맞추었는지 확인해보자."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[12,  0,  0],\n       [ 1, 13,  0],\n       [ 0,  0, 10]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, pred_nb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[11,  1,  0],\n       [ 1, 13,  0],\n       [ 0,  0, 10]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, pred_dtc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[12,  0,  0],\n       [ 0, 14,  0],\n       [ 0,  0, 10]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y, pred_rf)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "class imbalance가 어느 정도 있는 dataset을 사용하여 학습하였기 때문에, 학습된 결과에 대해서 precision과 recall의 조화평균인 f1-score를 사용하였다.\n",
    "\n",
    "f1-score를 사용할 때는 class imbalance 문제를 어느 정도 반영하기 위해, macro average보다는 micro average를 선택하였다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score\n",
      "naive bayes: 0.97\n",
      "decision tree: 0.94\n",
      "random forest: 1.00\n"
     ]
    }
   ],
   "source": [
    "print(\"F1-Score\")\n",
    "print(\"naive bayes: {0:.2f}\".format(f1_score(test_y, pred_nb, average='micro')))\n",
    "print(\"decision tree: {0:.2f}\".format(f1_score(test_y, pred_dtc, average='micro')))\n",
    "print(\"random forest: {0:.2f}\".format(f1_score(test_y, pred_rf, average='micro')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "실험 결과, RandomForest 방식의 분류가 가장 높은 성능을 보였음을 알 수 있다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "이번에는 수업시간에 배운 StratifiedKFold 방식을 사용하여 Cross validation을 사용해보겠다. 반복횟수는 5번으로 지정하고, K번마다 K개의 train dataset을 나누어 평가를 진행한다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Cross Validation - 1 ---------------\n",
      "naive bayes: 0.94\n",
      "decision tree: 0.94\n",
      "random forest: 0.94\n",
      "--------------- Cross Validation - 2 ---------------\n",
      "naive bayes: 0.97\n",
      "decision tree: 0.78\n",
      "random forest: 0.94\n",
      "--------------- Cross Validation - 3 ---------------\n",
      "naive bayes: 0.97\n",
      "decision tree: 0.89\n",
      "random forest: 0.97\n",
      "--------------- Cross Validation - 4 ---------------\n",
      "naive bayes: 0.94\n",
      "decision tree: 0.91\n",
      "random forest: 1.00\n",
      "--------------- Cross Validation - 5 ---------------\n",
      "naive bayes: 1.00\n",
      "decision tree: 0.86\n",
      "random forest: 1.00\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "n_iter = 0\n",
    "logr_score = []\n",
    "nb_score = []\n",
    "dtc_score = []\n",
    "rf_score = []\n",
    "for train_idx, test_idx in skf.split(df_wine, df_wine['target']):\n",
    "    n_iter += 1\n",
    "    train_X, test_X = wine_data[train_idx], wine_data[test_idx]\n",
    "    train_y, test_y = wine.target[train_idx], wine.target[test_idx]\n",
    "\n",
    "    nb.fit(train_X, train_y)\n",
    "    dtc.fit(train_X, train_y)\n",
    "    rf.fit(train_X, train_y)\n",
    "\n",
    "    pred_nb = nb.predict(test_X)\n",
    "    pred_dtc = dtc.predict(test_X)\n",
    "    pred_rf = rf.predict(test_X)\n",
    "\n",
    "    nb_score.append(f1_score(test_y, pred_nb, average='micro'))\n",
    "    dtc_score.append(f1_score(test_y, pred_dtc, average='micro'))\n",
    "    rf_score.append(f1_score(test_y, pred_rf, average='micro'))\n",
    "    print('--------------- Cross Validation - {} ---------------'.format(n_iter))\n",
    "    print(\"naive bayes: {0:.2f}\".format(f1_score(test_y, pred_nb, average='micro')))\n",
    "    print(\"decision tree: {0:.2f}\".format(f1_score(test_y, pred_dtc, average='micro')))\n",
    "    print(\"random forest: {0:.2f}\".format(f1_score(test_y, pred_rf, average='micro')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 평균 F1 Score - {} ---------------\n",
      "logistic regression: 0.97\n",
      "logistic regression: 0.88\n",
      "logistic regression: 0.97\n"
     ]
    }
   ],
   "source": [
    "print('--------------- 평균 F1 Score - {} ---------------')\n",
    "print(\"logistic regression: {0:.2f}\".format(np.mean(nb_score)))\n",
    "print(\"logistic regression: {0:.2f}\".format(np.mean(dtc_score)))\n",
    "print(\"logistic regression: {0:.2f}\".format(np.mean(rf_score)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
